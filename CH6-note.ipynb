{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(ISLR): there is no package called ‘ISLR’\n",
     "output_type": "error",
     "traceback": [
      "Error in library(ISLR): there is no package called ‘ISLR’\nTraceback:\n",
      "1. library(ISLR)",
      "2. stop(txt, domain = NA)"
     ]
    }
   ],
   "source": [
    "#Model Selection\n",
    "#================\n",
    "#\n",
    "#This is an R Markdown document. Markdown is a simple formatting syntax for authoring web pages,\n",
    "#and a very nice way of distributing an analysis. It has some very simple syntax rules.\n",
    "#\n",
    "#\n",
    "#```{r}\n",
    "library(ISLR)\n",
    "summary(Hitters)\n",
    "#```\n",
    "#There are some missing values here, so before we proceed we will remove them:\n",
    "#\n",
    "#```{r}\n",
    "Hitters=na.omit(Hitters)\n",
    "with(Hitters,sum(is.na(Salary)))\n",
    "#```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = x, srcfile = src): <text>:18:7: unexpected symbol\n17: #```\n18: There is\n          ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = x, srcfile = src): <text>:18:7: unexpected symbol\n17: #```\n18: There is\n          ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "#Best Subset regression\n",
    "#------------------------\n",
    "#We will now use the package `leaps` to evaluate all the best-subset models.\n",
    "#```{r}\n",
    "library(leaps)\n",
    "regfit.full=regsubsets(Salary~.,data=Hitters)\n",
    "summary(regfit.full)\n",
    "#```\n",
    "#It gives by default best-subsets up to size 8; lets increase that to 19, i.e. all the variables\n",
    "#```{r}\n",
    "regfit.full=regsubsets(Salary~.,data=Hitters, nvmax=19)\n",
    "reg.summary=summary(regfit.full)\n",
    "names(reg.summary)\n",
    "plot(reg.summary$cp,xlab=\"Number of Variables\",ylab=\"Cp\")\n",
    "which.min(reg.summary$cp)\n",
    "points(10,reg.summary$cp[10],pch=20,col=\"red\")\n",
    "#```\n",
    "There is a plot method for the `regsubsets`  object\n",
    "#```{r}\n",
    "plot(regfit.full,scale=\"Cp\")\n",
    "coef(regfit.full,10)\n",
    "#```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = \"forward\"): no se pudo encontrar la función \"regsubsets\"\n",
     "output_type": "error",
     "traceback": [
      "Error in regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = \"forward\"): no se pudo encontrar la función \"regsubsets\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "#Forward Stepwise Selection\n",
    "#--------------------------\n",
    "#Here we use the `regsubsets` function but specify the `method=\"forward\" option:\n",
    "#```{r}\n",
    "regfit.fwd=regsubsets(Salary~.,data=Hitters,nvmax=19,method=\"forward\")\n",
    "summary(regfit.fwd)\n",
    "plot(regfit.fwd,scale=\"Cp\")\n",
    "#```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): objeto 'Hitters' no encontrado\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): objeto 'Hitters' no encontrado\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "#Model Selection Using a Validation Set\n",
    "#---------------------------------------\n",
    "#Lets make a training and validation set, so that we can choose a good subset model.\n",
    "#We will do it using a slightly different approach from what was done in the the book.\n",
    "#```{r}\n",
    "dim(Hitters)\n",
    "set.seed(1)\n",
    "train=sample(seq(263),180,replace=FALSE)\n",
    "train\n",
    "regfit.fwd=regsubsets(Salary~.,data=Hitters[train,],nvmax=19,method=\"forward\")\n",
    "#```\n",
    "#Now we will make predictions on the observations not used for training. We know there are 19 models, so we set up some vectors to record the errors. We have to do a bit of work here, because there is no predict method for `regsubsets`.\n",
    "#```{r}\n",
    "val.errors=rep(NA,19)\n",
    "x.test=model.matrix(Salary~.,data=Hitters[-train,])# notice the -index!\n",
    "for(i in 1:19){\n",
    "  coefi=coef(regfit.fwd,id=i)\n",
    "  pred=x.test[,names(coefi)]%*%coefi\n",
    "  val.errors[i]=mean((Hitters$Salary[-train]-pred)^2)\n",
    "}\n",
    "plot(sqrt(val.errors),ylab=\"Root MSE\",ylim=c(300,400),pch=19,type=\"b\")\n",
    "points(sqrt(regfit.fwd$rss[-1]/180),col=\"blue\",pch=19,type=\"b\")\n",
    "legend(\"topright\",legend=c(\"Training\",\"Validation\"),col=c(\"blue\",\"black\"),pch=19)\n",
    "#```\n",
    "#As we expect, the training error goes down monotonically as the model gets bigger, but not so \n",
    "#for the validation error.\n",
    "\n",
    "#This was a little tedious - not having a predict method for `regsubsets`. So we will write one!\n",
    "#```{r}\n",
    "predict.regsubsets=function(object,newdata,id,...){\n",
    "  form=as.formula(object$call[[2]])\n",
    "  mat=model.matrix(form,newdata)\n",
    "  coefi=coef(object,id=id)\n",
    "  mat[,names(coefi)]%*%coefi\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in nrow(Hitters): objeto 'Hitters' no encontrado\n",
     "output_type": "error",
     "traceback": [
      "Error in nrow(Hitters): objeto 'Hitters' no encontrado\nTraceback:\n",
      "1. sample(rep(1:10, length = nrow(Hitters)))",
      "2. nrow(Hitters)"
     ]
    }
   ],
   "source": [
    "#Model Selection by Cross-Validation\n",
    "#-----------------------------------\n",
    "#We will do 10-fold cross-validation. Its really easy!\n",
    "#```{r}\n",
    "set.seed(11)\n",
    "folds=sample(rep(1:10,length=nrow(Hitters)))\n",
    "folds\n",
    "table(folds)\n",
    "cv.errors=matrix(NA,10,19)\n",
    "for(k in 1:10){\n",
    "  best.fit=regsubsets(Salary~.,data=Hitters[folds!=k,],nvmax=19,method=\"forward\")\n",
    "  for(i in 1:19){\n",
    "    pred=predict(best.fit,Hitters[folds==k,],id=i)\n",
    "    cv.errors[k,i]=mean( (Hitters$Salary[folds==k]-pred)^2)\n",
    "  }\n",
    "}\n",
    "rmse.cv=sqrt(apply(cv.errors,2,mean))\n",
    "plot(rmse.cv,pch=19,type=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: Matrix\n",
      "Loading required package: foreach\n",
      "Loaded glmnet 2.0-13\n",
      "\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in terms.formula(object, data = data): objeto 'Hitters' no encontrado\n",
     "output_type": "error",
     "traceback": [
      "Error in terms.formula(object, data = data): objeto 'Hitters' no encontrado\nTraceback:\n",
      "1. model.matrix(Salary ~ . - 1, data = Hitters)",
      "2. model.matrix.default(Salary ~ . - 1, data = Hitters)",
      "3. terms(object, data = data)",
      "4. terms.formula(object, data = data)"
     ]
    }
   ],
   "source": [
    "#Ridge Regression and the Lasso\n",
    "#-------------------------------\n",
    "#We will use the package `glmnet`, which does not use the model formula language, so we will set up an `x` and `y`.\n",
    "#```{r}\n",
    "library(glmnet)\n",
    "x=model.matrix(Salary~.-1,data=Hitters) \n",
    "y=Hitters$Salary\n",
    "\n",
    "#First we will fit a ridge-regression model. This is achieved by calling `glmnet` with `alpha=0` (see the helpfile). There is also a `cv.glmnet` function which will do the cross-validation for us. \n",
    "#```{r}\n",
    "fit.ridge=glmnet(x,y,alpha=0)\n",
    "plot(fit.ridge,xvar=\"lambda\",label=TRUE)\n",
    "cv.ridge=cv.glmnet(x,y,alpha=0)\n",
    "plot(cv.ridge)\n",
    "\n",
    "#Now we fit a lasso model; for this we use the default `alpha=1`\n",
    "#```{r}\n",
    "fit.lasso=glmnet(x,y)\n",
    "plot(fit.lasso,xvar=\"lambda\",label=TRUE)\n",
    "cv.lasso=cv.glmnet(x,y)\n",
    "plot(cv.lasso)\n",
    "coef(cv.lasso)\n",
    "\n",
    "#Suppose we want to use our earlier train/validation division to select the `lambda` for the lasso.\n",
    "# This is easy to do.\n",
    "#```{r}\n",
    "lasso.tr=glmnet(x[train,],y[train])\n",
    "lasso.tr\n",
    "pred=predict(lasso.tr,x[-train,])\n",
    "dim(pred)\n",
    "rmse= sqrt(apply((y[-train]-pred)^2,2,mean))\n",
    "plot(log(lasso.tr$lambda),rmse,type=\"b\",xlab=\"Log(lambda)\")\n",
    "lam.best=lasso.tr$lambda[order(rmse)[1]]\n",
    "lam.best\n",
    "coef(lasso.tr,s=lam.best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
